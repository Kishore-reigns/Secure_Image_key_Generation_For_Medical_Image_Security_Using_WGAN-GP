{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BWezU18AjOSb"},"outputs":[],"source":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MqubOg3OjUqf","outputId":"c324095b-6a80-4ea0-e9ce-1cca814c1d6f","executionInfo":{"status":"ok","timestamp":1741147798696,"user_tz":-330,"elapsed":16876,"user":{"displayName":"KISHORE reigns","userId":"05118561739814802865"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","# kishresun2016@gmail.com -> /content/drive/MyDrive/Sem6/CIP_Team6_2025/Transformation_zip.zip\n","# kishreigns@gamil.com -> /content/drive/MyDrive/Transformation_zip.zip\n","# malarvannanm11@gmail.com -> /content/drive/MyDrive/Transformation_zip.zip\n","# maanasa -> /content/drive/MyDrive/CIP/Transformation.zip\n","\n","trans_zip_path = \"/content/drive/MyDrive/Sem6/CIP_Team6_2025/Transformation_zip.zip\"\n","source_zip_path = \"/content/drive/MyDrive/Sem6/CIP_Team6_2025/source.zip\" #copy path of source domain from cip folder\n","trans_extract_path = \"/content/transformation\"\n","source_extract_path = \"/content/source\"\n","drive_checkpoint_link = \"/content/drive/MyDrive/Sem6/CIP_Team6_2025/WGAN_GP_working/deepkeygen_checkpoint.pth\"\n","with zipfile.ZipFile(trans_zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(trans_extract_path)\n","\n","print(f\"Transformation domain extracted successfully: {trans_extract_path}\")\n","\n","with zipfile.ZipFile(source_zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(source_extract_path)\n","print(f\"Souce domain extracted successfully: {source_extract_path}\")\n"],"metadata":{"id":"MewAy1EXjhcY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"84db48be-668d-431d-a144-9cb725fb8598","executionInfo":{"status":"ok","timestamp":1741148479560,"user_tz":-330,"elapsed":12930,"user":{"displayName":"KISHORE reigns","userId":"05118561739814802865"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformation domain extracted successfully: /content/transformation\n","Souce domain extracted successfully: /content/source\n"]}]},{"cell_type":"code","source":["import torch_xla\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, ConcatDataset , Dataset\n","from torchvision import transforms, datasets, utils\n","from PIL import Image\n","import os\n","import kagglehub\n","import torch.autograd as autograd\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.parallel_loader as pl\n","\n","import torch.nn.functional as F\n","import csv\n","# Reduce batch size to save memory\n","batch_size = 4  # Adjusted from 8\n","\n","# Move data to TPU-efficient loader\n","#source_loader = pl.MpDeviceLoader(\n"," #   DataLoader(source_dataset, batch_size=batch_size, shuffle=True), xm.xla_device()\n","#)\n","#transform_loader = pl.MpDeviceLoader(\n","#    DataLoader(transform_dataset, batch_size=batch_size, shuffle=True), xm.xla_device()\n","#)\n","\n","# Adjust training function to include xm.mark_step()\n","def train_deepkeygen(generator, critic, source_loader, transform_loader, num_epochs, lr, device):\n","    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n","    optimizer_d = optim.Adam(critic.parameters(), lr=lr, betas=(0.5, 0.9))\n","    lambda_gp = 5\n","    critic_iterations = 3\n","\n","    start_epoch = load_checkpoint(generator, critic, optimizer_g, optimizer_d)\n","    csv_path = \"/content/drive/MyDrive/Sem6/CIP_Team6_2025/WGAN_GP_working/metrices.csv\"\n","    #/content/drive/MyDrive/Sem6/CIP_Team6_2025/WGAN_GP_working/metrices.csv\n","    #/content/drive/MyDrive/metrices/metrices.csv\n","    if not os.path.exists(csv_path):\n","        with open(csv_path, \"w\", newline=\"\") as file:\n","            writer = csv.writer(file)\n","            writer.writerow([\"Epoch\", \"Generator Loss\", \"Critic Loss\", \"Wasserstein Distance\", \"Gradient Penalty\", \"D_real\", \"D_fake\", \"Gen_Acc\", \"Critic_Acc\"])\n","\n","    for epoch in range(start_epoch, num_epochs):\n","        print(f\"Epoch: {epoch + 1}/{num_epochs}\", flush=True)\n","        total_gen_correct = 0\n","        total_critic_correct = 0\n","        total_samples = 0\n","        for source_batch, transform_batch in zip(source_loader, transform_loader):\n","            source_imgs, _ = source_batch  # Extract images, ignore labels\n","            transform_imgs, _ = transform_batch  # Extract images, ignore labels\n","\n","            source_imgs = source_imgs.to(device)\n","            transform_imgs = transform_imgs.to(device)\n","\n","            for _ in range(critic_iterations):\n","                fake_imgs = generator(source_imgs).detach()\n","                real_scores = critic(transform_imgs)\n","                fake_scores = critic(fake_imgs)\n","\n","                real_loss = real_scores.mean()\n","                fake_loss = fake_scores.mean()\n","                gp = compute_gradient_penalty(critic, transform_imgs, fake_imgs, device)\n","                critic_loss = fake_loss - real_loss + lambda_gp * gp\n","\n","                optimizer_d.zero_grad()\n","                critic_loss.backward()\n","                xm.optimizer_step(optimizer_d)\n","                xm.mark_step()  # Free TPU memory\n","\n","                total_samples += real_scores.size(0) + fake_scores.size(0)\n","                total_critic_correct += (real_scores > 0).sum().item() + (fake_scores < 0).sum().item()\n","\n","            fake_imgs = generator(source_imgs)\n","            fake_scores = critic(fake_imgs)\n","            generator_loss = -fake_scores.mean()\n","\n","            optimizer_g.zero_grad()\n","            generator_loss.backward()\n","            xm.optimizer_step(optimizer_g)\n","            xm.mark_step()  # Free TPU memory\n","\n","            total_gen_correct += (fake_scores > 0).sum().item()\n","\n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss D: {critic_loss.item()}, Loss G: {generator_loss.item()}\", flush=True)\n","        gen_acc = (total_gen_correct / total_samples) * 100\n","        critic_acc = (total_critic_correct / total_samples) * 100\n","        wasserstein_distance = real_loss.item() - fake_loss.item()\n","\n","        with open(csv_path, \"a\", newline=\"\") as file:\n","            writer = csv.writer(file)\n","            writer.writerow([epoch+1, generator_loss.item(), critic_loss.item(), wasserstein_distance, gp.item(), real_loss.item(), fake_loss.item(), gen_acc, critic_acc])\n","\n","        if (epoch + 1) % 5 == 0:\n","            save_checkpoint(generator, critic, optimizer_g, optimizer_d, epoch)\n","        sample_image, _ = next(iter(source_loader))  # Extract one batch\n","        sample_image = sample_image[:1]  # Select only one image (batch size 1)\n","        save_generated_images(generator, epoch, device, sample_image)\n","\n","    print(\"[+] Training ended\", flush=True)\n","\n","\n","\n","data = []\n","drive_checkpoint_link = \"/content/drive/MyDrive/Sem6/CIP_Team6_2025/WGAN_GP_working/deepkeygen_checkpoint.pth\"\n","# /checkpoints/deepkeygen_checkpoint.pth\n","#/content/drive/MyDrive/checkpoints/deepkeygen_checkpoint.pth\n","\n","# Function to download multiple datasets\n","def download_datasets(dataset_list):\n","    dataset_dirs = [kagglehub.dataset_download(dataset) for dataset in dataset_list]\n","    return dataset_dirs\n","\n","# Function to load multiple datasets into a single DataLoader\n","def load_multiple_datasets(data_dirs, transform, batch_size):\n","    datasets_list = [datasets.ImageFolder(data_dir, transform=transform) for data_dir in data_dirs]\n","    combined_dataset = ConcatDataset(datasets_list)\n","    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n","\n","def save_checkpoint(generator, critic, optimizer_g, optimizer_d, epoch, filepath=drive_checkpoint_link):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'generator_state_dict': generator.state_dict(),\n","        'critic_state_dict': critic.state_dict(),\n","        'optimizer_g_state_dict': optimizer_g.state_dict(),\n","        'optimizer_d_state_dict': optimizer_d.state_dict()\n","    }\n","    torch.save(checkpoint, filepath)\n","    print(f\"[+]Checkpoint saved at epoch {epoch+1}\")\n","\n","def load_checkpoint(generator, critic, optimizer_g, optimizer_d, filepath=drive_checkpoint_link, device=None):\n","    if os.path.exists(filepath) and os.path.getsize(filepath) > 0: # Check if file exists and has content\n","        try:\n","            checkpoint = torch.load(filepath, map_location=device)\n","            generator.load_state_dict(checkpoint['generator_state_dict'])\n","            critic.load_state_dict(checkpoint['critic_state_dict'])\n","            optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n","            optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n","            start_epoch = checkpoint['epoch'] + 1\n","            print(f\"[+]Resuming training from epoch {start_epoch}\")\n","        except RuntimeError as e:\n","            print(f\"[-]Error loading checkpoint: {e}\") # Print error message if loading fails\n","            start_epoch = 0 # Start from epoch 0 if loading fails\n","            print(\"[-]Starting training from scratch due to checkpoint loading error.\")\n","    else:\n","        start_epoch = 0\n","        print(\"[!]No checkpoint found, starting training from scratch.\")\n","\n","    return start_epoch\n","\n","\n","class PairedMedicalDataset(Dataset):\n","    def __init__(self, source_dir, transform_dir, transform):\n","        self.source_images = sorted(os.listdir(source_dir))\n","        self.transform_images = sorted(os.listdir(transform_dir))\n","        self.source_dir = source_dir\n","        self.transform_dir = transform_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.source_images)\n","\n","    def __getitem__(self, idx):\n","        source_path = os.path.join(self.source_dir, self.source_images[idx])\n","        transform_path = os.path.join(self.transform_dir, self.transform_images[idx])\n","\n","        source_img = Image.open(source_path).convert(\"RGB\")\n","        transform_img = Image.open(transform_path).convert(\"RGB\")\n","\n","        source_img = self.transform(source_img)\n","        transform_img = self.transform(transform_img)\n","\n","        return source_img, transform_img\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","\n","            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.Tanh()  # Output image [-1,1] range\n","        )\n","\n","    def forward(self, img):\n","        x = self.encoder(img)\n","        x = self.decoder(x)\n","        return x\n","\n","\n","\n","# Critic network\n","class Critic(nn.Module):\n","    def __init__(self):\n","        super(Critic, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(128, 1, 4, 1, 0, bias=False)\n","        )\n","\n","    def forward(self, img):\n","        return self.model(img).view(-1)\n","\n","def compute_gradient_penalty(critic, real_samples, fake_samples, device):\n","    min_batch_size = min(real_samples.size(0), fake_samples.size(0))\n","    real_samples = real_samples[:min_batch_size]\n","    fake_samples = fake_samples[:min_batch_size]\n","\n","    if real_samples.shape != fake_samples.shape:\n","      print(f\"Shape mismatch: real_samples: {real_samples.shape}, fake_samples: {fake_samples.shape}\")\n","\n","    # Create random alpha values for interpolation\n","    alpha = torch.rand(real_samples.shape[0], 1, 1, 1, device=device)  # Random alpha for each image in the batch\n","\n","    # Expand alpha to match the spatial dimensions of the image (8, 3, 256, 256)\n","    alpha = alpha.view(real_samples.shape[0], 1, 1, 1).expand_as(real_samples)\n","\n","\n","\n","\n","\n","    # Interpolate between real and fake samples\n","    interpolates = alpha * real_samples + (1 - alpha) * fake_samples\n","    interpolates.requires_grad_(True)\n","\n","    # Pass the interpolates through the critic\n","    critic_interpolates = critic(interpolates)\n","\n","    # Compute gradients of the critic's output w.r.t. interpolates\n","    grad_outputs = torch.ones_like(critic_interpolates, device=device)\n","\n","    gradients = torch.autograd.grad(\n","        outputs=critic_interpolates,\n","        inputs=interpolates,\n","        grad_outputs=grad_outputs,\n","        create_graph=True,\n","        retain_graph=True,\n","        only_inputs=True\n","    )[0]\n","\n","    # Flatten the gradients and compute the gradient penalty\n","    gradients = gradients.view(gradients.shape[0], -1)\n","    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","\n","    return gradient_penalty\n","\n","\n","\n","def save_generated_images(generator, epoch, device, sample_image):\n","    generator.eval()\n","    with torch.no_grad():\n","        sample_image = sample_image.to(device)  # Ensure it's on the right device\n","        fake_img = generator(sample_image).squeeze(0)  # Shape: (3, 256, 256)\n","\n","        os.makedirs(\"generated_images\", exist_ok=True)\n","        image_path = f\"generated_images/epoch_{epoch}.png\"\n","\n","        # Save image with correct size\n","        utils.save_image(fake_img, image_path, normalize=True)\n","\n","    generator.train()\n","\n","\n","\n","# Main script\n","if __name__ == \"__main__\":\n","    print(f\"[+] Current working directory: {os.getcwd()}\")\n","\n","    device = xm.xla_device()  # Use TPU device\n","    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n","    #device = torch.device(\"cuda\")\n","    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n","    print(f\"[+] Using device: {device}\")\n","\n","    #csvpath = \"/content/loss.csv\"\n","\n","    #source_datasets = [\"raddar/tuberculosis-chest-xrays-montgomery\", \"masoudnickparvar/brain-tumor-mri-dataset\"]\n","    #source_data_dirs = download_datasets(source_datasets)  # Download datasets from Kaggle\n","\n","    # **Define Paths for Source & Transformed Data**\n","    source_dir = \"/content/source\"  # Directory containing original medical images\n","    transform_dir = \"/content/transformation\"  # Directory containing transformed keys\n","\n","    # **Load Datasets**\n","    batch_size = 8\n","    num_epochs = 100\n","    lr = 0.0002\n","    transform = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))\n","    ])\n","    source_dataset = datasets.ImageFolder(source_dir, transform=transform)\n","    transform_dataset = datasets.ImageFolder(transform_dir, transform=transform)\n","\n","    # **Create Data Loaders**\n","    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n","    transform_loader = DataLoader(transform_dataset, batch_size=batch_size, shuffle=True)\n","\n","    print(\"[+] Datasets loaded successfully!\")\n","\n","\n","    generator = Generator().to(device)\n","    critic = Critic().to(device)\n","\n","    print(\"[+] Training begins\")\n","    train_deepkeygen(generator, critic, source_loader, transform_loader, num_epochs, lr, device)\n","    print(\"[+] Training ended\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":819},"id":"WR2LewzskWMr","outputId":"13ea10c7-0e1c-4290-d12f-f68ee65e2d7e","executionInfo":{"status":"error","timestamp":1741151153284,"user_tz":-330,"elapsed":2667793,"user":{"displayName":"KISHORE reigns","userId":"05118561739814802865"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[+] Current working directory: /content\n","[+] Using device: xla:0\n","[+] Datasets loaded successfully!\n","[+] Training begins\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-6d98dbd2905e>:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(filepath, map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["[+]Resuming training from epoch 15\n","Epoch: 16/100\n","Epoch [16/100], Loss D: 0.03657171502709389, Loss G: 0.005026935134083033\n","Epoch: 17/100\n","Epoch [17/100], Loss D: 0.13608409464359283, Loss G: 0.00901350099593401\n","Epoch: 18/100\n","Epoch [18/100], Loss D: 0.010600322857499123, Loss G: 0.004307465627789497\n","Epoch: 19/100\n","Epoch [19/100], Loss D: 0.004525856580585241, Loss G: 0.011366089805960655\n","Epoch: 20/100\n","Epoch [20/100], Loss D: 0.01976926624774933, Loss G: -0.02192833088338375\n","[+]Checkpoint saved at epoch 20\n","Epoch: 21/100\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6d98dbd2905e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[+] Training begins\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mtrain_deepkeygen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[+] Training ended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-6d98dbd2905e>\u001b[0m in \u001b[0;36mtrain_deepkeygen\u001b[0;34m(generator, critic, source_loader, transform_loader, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mtotal_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreal_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfake_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mtotal_critic_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_scores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfake_scores\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mfake_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}